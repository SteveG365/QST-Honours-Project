{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d101c227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from qiskit.quantum_info import DensityMatrix, random_density_matrix\n",
    "from qiskit.quantum_info.operators import Operator\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import torch\n",
    "#tensorflow imports\n",
    "from tensorflow import keras\n",
    "from keras import layers, losses, Model\n",
    "import logging\n",
    "from functools import reduce\n",
    "from itertools import product\n",
    "import time\n",
    "tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1d488",
   "metadata": {},
   "source": [
    "# 1.0) Comparing runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3aa3c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fidelity\n",
    "def fidelity(rho1, rho2):\n",
    "    sqrt_rho1 = sqrtm(rho1)\n",
    "    F = np.trace(sqrtm(sqrt_rho1 @ rho2 @ sqrt_rho1))\n",
    "    return np.real(F)**2\n",
    "\n",
    "def kron_all(mats):\n",
    "    return reduce(np.kron, mats)\n",
    "\n",
    "# Map the cholesky parameterised vector to a PSD trace 1 density matrix\n",
    "def params_to_rho(d, params):\n",
    "    L = np.zeros((d,d), dtype=complex)\n",
    "    idx = 0\n",
    "    # diagonal entries (real, positive)\n",
    "    for i in range(d):\n",
    "        L[i, i] = params[idx]\n",
    "        idx += 1\n",
    "    # lower-triangular off-diagonals (real + imag)\n",
    "    for i in range(1, d):\n",
    "        for j in range(i):\n",
    "            re = params[idx]; im = params[idx+1]\n",
    "            L[i, j] = re + 1j * im\n",
    "            idx += 2\n",
    "    rho = L @ L.conj().T\n",
    "    return rho / np.trace(rho)\n",
    "\n",
    "# Single-qubit unitaries\n",
    "def build_single_qubit_Us():\n",
    "    X_cols = [np.array([1, 1])/np.sqrt(2),\n",
    "              np.array([1,-1])/np.sqrt(2)]\n",
    "    Y_cols = [np.array([1, 1j])/np.sqrt(2),\n",
    "              np.array([1,-1j])/np.sqrt(2)] \n",
    "    Z_cols = [np.array([1,0]), np.array([0,1])]\n",
    "    return {'X': np.column_stack(X_cols),\n",
    "            'Y': np.column_stack(Y_cols),\n",
    "            'Z': np.column_stack(Z_cols)}\n",
    "\n",
    "# POVM builder for arbitrary d = 2^n\n",
    "def build_povm(d):\n",
    "    n_qubits = int(np.log2(d))\n",
    "    Us1 = build_single_qubit_Us()\n",
    "    settings = []\n",
    "    for bases in product(['X','Y','Z'], repeat=n_qubits):\n",
    "        U = kron_all([Us1[b] for b in bases])\n",
    "        settings.append(U)\n",
    "\n",
    "    # computational-basis projectors\n",
    "    proj = []\n",
    "    for m in range(d):\n",
    "        P = np.zeros((d, d), dtype=complex)\n",
    "        P[m, m] = 1.0\n",
    "        proj.append(P)\n",
    "\n",
    "    # rotated projectors\n",
    "    E = []\n",
    "    for U in settings:\n",
    "        U_dag = U.conj().T\n",
    "        for P in proj:\n",
    "            E.append(U @ P @ U_dag)\n",
    "    return E  \n",
    "\n",
    "# Negative log-likelihood\n",
    "def neg_log_likelihood(params, counts, d, E):\n",
    "    rho = params_to_rho(d, params)\n",
    "    probs = np.array([np.trace(Ei @ rho).real for Ei in E])\n",
    "    probs = np.clip(probs, 1e-15, 1.0)\n",
    "    return -np.sum(counts * np.log(probs))\n",
    "\n",
    "# MLE with runtime\n",
    "def MLE_Avg_fidelity_with_runtime(data, X, y, d):\n",
    "    shots = data['shots_per_basis']\n",
    "    counts = (X * shots).astype(int) \n",
    "\n",
    "    N = X.shape[0]\n",
    "    rho_est = np.zeros((N, d, d), dtype=complex)\n",
    "    fidelities = np.zeros(N)\n",
    "    runtimes = []\n",
    "\n",
    "    # build POVM once for this d\n",
    "    E = build_povm(d)\n",
    "\n",
    "    for i in range(N):\n",
    "        # initial guess: uniform identity\n",
    "        init = np.zeros(d*d)\n",
    "        init[:d] = np.sqrt(1/d)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        res = minimize(\n",
    "            neg_log_likelihood, init, args=(counts[i], d, E),\n",
    "            method='L-BFGS-B',\n",
    "            options={'maxiter': 50}\n",
    "        )\n",
    "        end = time.perf_counter()\n",
    "        \n",
    "        runtimes.append(end - start)\n",
    "\n",
    "        # reconstruct density matrix\n",
    "        rho_est[i] = params_to_rho(d, res.x)\n",
    "        fidelities[i] = fidelity(rho_est[i], y[i])\n",
    "\n",
    "    total_runtime = np.sum(runtimes)\n",
    "    avg_runtime   = np.mean(runtimes)\n",
    "    return total_runtime, avg_runtime, np.mean(fidelities)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rho_to_alpha(rho):\n",
    "    \"\"\"\n",
    "    Convert a (d x d) density matrix rho into its \"Cholesky parameters\" alpha.\n",
    "    \"\"\"\n",
    "    L = np.linalg.cholesky(rho + 1e-14 * np.eye(rho.shape[0]))\n",
    "    d = rho.shape[0]\n",
    "    alpha = []\n",
    "    # diagonal (real, >0)\n",
    "    for i in range(d):\n",
    "        alpha.append(np.real(L[i, i]))\n",
    "    # strictly lower triangle (real + imag)\n",
    "    for i in range(1, d):\n",
    "        for j in range(i):\n",
    "            alpha.append(np.real(L[i, j]))\n",
    "            alpha.append(np.imag(L[i, j]))\n",
    "    return np.array(alpha, dtype=np.float32)\n",
    "\n",
    "def fidelity(rho1, rho2):\n",
    "    A = sqrtm(rho1)\n",
    "    return np.real(np.trace(sqrtm(A @ rho2 @ A)))**2\n",
    "\n",
    "\n",
    "def tf_sqrtm_psd(A):\n",
    "    \"\"\"\n",
    "    Compute principal sqrt of Hermitian PSD A (batch,d,d) via eigendecomposition.\n",
    "    \"\"\"\n",
    "    \n",
    "    eigvals, eigvecs = tf.linalg.eigh(A)\n",
    "\n",
    "    eigvals = tf.math.real(eigvals)\n",
    "    eigvals = tf.clip_by_value(eigvals, 0.0, tf.reduce_max(eigvals))\n",
    "    sqrtvals = tf.sqrt(eigvals)\n",
    "    D = tf.cast(tf.linalg.diag(sqrtvals), tf.complex64)\n",
    "    \n",
    "    return eigvecs @ D @ tf.linalg.adjoint(eigvecs)\n",
    "\n",
    "def tf_alpha_to_rho(alpha, d):\n",
    "    \"\"\"\n",
    "    Map real alpha (batch, N_alpha) -> complex density matrices (batch, d, d).\n",
    "    Enforces positivity via a softplus on the Cholesky diag.\n",
    "    \"\"\"\n",
    "    batch = tf.shape(alpha)[0]\n",
    "    # split diag vs off-diag\n",
    "    raw_diag = alpha[:, :d] \n",
    "    off_vals  = alpha[:, d:] \n",
    "\n",
    "    # start zero L\n",
    "    L = tf.zeros((batch, d, d), tf.complex64)\n",
    "\n",
    "    diag_pos = tf.nn.softplus(raw_diag) + 1e-6 \n",
    "    diag_c   = tf.cast(diag_pos, tf.complex64)\n",
    "    L = tf.linalg.set_diag(L, diag_c)\n",
    "\n",
    "    idx = 0\n",
    "    for i in range(1, d):\n",
    "        for j in range(i):\n",
    "            re = off_vals[:, idx]\n",
    "            im = off_vals[:, idx+1]\n",
    "            idx += 2\n",
    "            cij = (tf.cast(re, tf.complex64)\n",
    "                   + 1j * tf.cast(im, tf.complex64))\n",
    "            cij = tf.reshape(cij, (batch, 1, 1))\n",
    "\n",
    "            # mask with a one-hot at (i,j)\n",
    "            flat = tf.one_hot(i*d + j, d*d, dtype=tf.complex64)\n",
    "            mask = tf.reshape(flat, (d, d))[None, :, :]\n",
    "\n",
    "            L = L + cij * mask\n",
    "\n",
    "    rho = L @ tf.linalg.adjoint(L)  \n",
    "    tr  = tf.linalg.trace(rho)\n",
    "    return rho / tr[:, None, None]\n",
    "\n",
    "def make_fidelity_loss(d):\n",
    "    def fidelity_loss(alpha_true, alpha_pred):\n",
    "        rho_t = tf_alpha_to_rho(alpha_true, d)\n",
    "        rho_p = tf_alpha_to_rho(alpha_pred, d)\n",
    "\n",
    "        # tiny regularizer to guard numeric issues\n",
    "        I = tf.eye(d, dtype=tf.complex64)[None, :, :]\n",
    "        rho_t = rho_t + 1e-8 * I\n",
    "        rho_p = rho_p + 1e-8 * I\n",
    "\n",
    "        sqrt_t = tf_sqrtm_psd(rho_t)\n",
    "        inter  = sqrt_t @ (rho_p @ sqrt_t)\n",
    "        s_mat  = tf_sqrtm_psd(inter)\n",
    "\n",
    "        tr_s = tf.linalg.trace(s_mat)\n",
    "        F = tf.abs(tr_s)**2\n",
    "        return tf.reduce_mean(1.0 - F)\n",
    "    return fidelity_loss\n",
    "\n",
    "def make_hybrid_loss(d, lam=0.8):\n",
    "    \"\"\"\n",
    "    Hybrid loss = lam * MSE + (1 - lam) * (1 - fidelity).\n",
    "    \"\"\"\n",
    "    fid_loss_fn = make_fidelity_loss(d)\n",
    "\n",
    "    def hybrid_loss(alpha_true, alpha_pred):\n",
    "        # MSE on the Cholesky parameters\n",
    "        mse = tf.reduce_mean(tf.square(alpha_true - alpha_pred))\n",
    "\n",
    "        # fidelity loss already = mean(1 - F)\n",
    "        phys = fid_loss_fn(alpha_true, alpha_pred)\n",
    "\n",
    "        return lam * mse + (1.0 - lam) * phys\n",
    "\n",
    "    return hybrid_loss\n",
    "\n",
    "def alpha_to_rho_batch(d, alpha):\n",
    "    \"\"\"Convert batch of alpha vectors to density matrices using Cholesky.\"\"\"\n",
    "    N = alpha.shape[0]\n",
    "    rho = np.zeros((N, d, d), dtype=np.complex64)\n",
    "    for i in range(N):\n",
    "        a = alpha[i]\n",
    "        L = np.zeros((d, d), dtype=np.complex64)\n",
    "        idx = 0\n",
    "        for j in range(d):\n",
    "            L[j, j] = a[idx]\n",
    "            idx += 1\n",
    "        for j in range(1, d):\n",
    "            for k in range(j):\n",
    "                re = a[idx]\n",
    "                im = a[idx + 1]\n",
    "                L[j, k] = re + 1j * im\n",
    "                idx += 2\n",
    "        rho_i = L @ L.conj().T\n",
    "        rho[i] = rho_i / np.trace(rho_i)\n",
    "    return rho\n",
    "\n",
    "def train_NN_fidelity(X, y, lam=0.8, epochs=30):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=2)\n",
    "\n",
    "    alphas = np.stack([rho_to_alpha(y[i]) for i in range(len(y))], axis=0)\n",
    "    d = y_train.shape[1]\n",
    "\n",
    "    alphas_train = np.stack([rho_to_alpha(rho) for rho in y_train], axis=0)\n",
    "    N_alpha = alphas_train.shape[1]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(N_alpha),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=make_hybrid_loss(d, lam=lam),\n",
    "    )\n",
    "\n",
    "    # fit\n",
    "    history = model.fit(\n",
    "        X_train, alphas_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "    )\n",
    "\n",
    "    alpha_pred = model.predict(X_test)      \n",
    "    rho_pred = alpha_to_rho_batch(d, alpha_pred)  \n",
    "\n",
    "    # Compute fidelities\n",
    "    fidelities = np.array([\n",
    "        fidelity(rho_pred[i], y_test[i])\n",
    "        for i in range(len(rho_pred))\n",
    "    ])\n",
    "\n",
    "    # Summary\n",
    "    mean_fid = np.mean(fidelities)\n",
    "    std_fid = np.std(fidelities)\n",
    "    return(mean_fid, std_fid)\n",
    "\n",
    "\n",
    "def make_hybrid_loss(d, lam):\n",
    "    lam = tf.convert_to_tensor(lam, dtype=tf.float32) \n",
    "    fid_loss_fn = make_fidelity_loss(d)\n",
    "    def hybrid_loss(alpha_true, alpha_pred):\n",
    "        mse = tf.reduce_mean(tf.square(alpha_true - alpha_pred))\n",
    "        phys = fid_loss_fn(alpha_true, alpha_pred)\n",
    "        return lam * mse + (1.0 - lam) * phys\n",
    "\n",
    "    return hybrid_loss\n",
    "\n",
    "class LambdaAnnealer(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, lam_var, lam_start, lam_end, epochs):\n",
    "        super().__init__()\n",
    "        self.lam_var = lam_var\n",
    "        self.lam_start = float(lam_start)\n",
    "        self.lam_end = float(lam_end)\n",
    "        self.epochs = int(epochs)\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.lam_var.assign(self.lam_start)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        t = epoch / max(1, self.epochs - 1)\n",
    "        lam = self.lam_start + t * (self.lam_end - self.lam_start)\n",
    "        self.lam_var.assign(lam)\n",
    "        if logs is not None:\n",
    "            logs['lambda'] = float(lam)\n",
    "\n",
    "def train_anneal_NN(X, y, mle_idx, lam_start=0.95, lam_end=0.10, epochs=30):\n",
    "\n",
    "    X_test = X[mle_idx]; y_test = y[mle_idx]\n",
    "    X_train = np.delete(X, mle_idx, axis=0)\n",
    "    y_train = np.delete(y, mle_idx, axis=0)\n",
    "\n",
    "    d = y_train.shape[1]\n",
    "    alphas_train = np.stack([rho_to_alpha(rho) for rho in y_train], axis=0)\n",
    "    N_alpha = alphas_train.shape[1]\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.GaussianNoise(0.02),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(512, activation=\"gelu\"),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        tf.keras.layers.Dense(512, activation=\"gelu\"),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        tf.keras.layers.Dense(512, activation=\"gelu\"),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "        tf.keras.layers.Dense(256, activation=\"gelu\"),\n",
    "        tf.keras.layers.LayerNormalization(),\n",
    "\n",
    "        tf.keras.layers.Dense(N_alpha),\n",
    "    ])\n",
    "\n",
    "    lam_var = tf.Variable(lam_start, trainable=False, dtype=tf.float32, name=\"lambda_weight\")\n",
    "\n",
    "    opt = tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-4, clipnorm=1.0)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=make_hybrid_loss(d, lam=lam_var),\n",
    "    )\n",
    "\n",
    "    class ValFidelity(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, Xv, yv, every=1, k=512):\n",
    "            super().__init__()\n",
    "            self.idx = np.random.RandomState(42).choice(len(Xv), size=min(k, len(Xv)), replace=False)\n",
    "            self.Xv = Xv[self.idx]\n",
    "            self.yv = yv[self.idx]\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            alpha_pred = self.model(self.Xv, training=False).numpy()\n",
    "            rho_pred = alpha_to_rho_batch(d, alpha_pred)\n",
    "            F = np.mean([fidelity(rho_pred[i], self.yv[i]) for i in range(len(rho_pred))])\n",
    "            if logs is not None:\n",
    "                logs['val_fidelity'] = float(F)\n",
    "            print(f\"Epoch {epoch+1}: val_fidelity={F:.4f}\")\n",
    "\n",
    "    fid_cb = ValFidelity(X_test, y_test, every=1, k=512)\n",
    "\n",
    "    cb = [\n",
    "        LambdaAnnealer(lam_var, lam_start=lam_start, lam_end=lam_end, epochs=epochs),\n",
    "        fid_cb,\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, alphas_train,\n",
    "        validation_split=0.1,\n",
    "        epochs=epochs,\n",
    "        batch_size=128,\n",
    "        callbacks=cb,\n",
    "        verbose=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    alpha_pred = model.predict(X_test, verbose=0)\n",
    "    rho_pred = alpha_to_rho_batch(d, alpha_pred)\n",
    "\n",
    "    fidelities = np.array([fidelity(rho_pred[i], y_test[i]) for i in range(len(rho_pred))])\n",
    "    return np.mean(fidelities), np.std(fidelities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae8c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "\n",
    "#2 qubit\n",
    "data_2q = np.load('../datasets/2_qubit_shots/2q_1k_.npz')\n",
    "X_2q = data_2q['counts'] / data_2q['shots_per_basis']; y_2q = data_2q['states']\n",
    "\n",
    "#3 qubit\n",
    "data_3q = np.load('../datasets/3_qubit_shots/3q_1000.npz')\n",
    "X_3q = data_3q['counts'] / data_3q['shots_per_basis']; y_3q = data_3q['states']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f7ef10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "mle_indices = rng.choice(100_000, size=1, replace=False)\n",
    "results_2q = MLE_Avg_fidelity_with_runtime(data_2q, X_2q[mle_indices], y_2q[mle_indices], d=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8333bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1690.9488750388846"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_3q = MLE_Avg_fidelity_with_runtime(data_3q, X_3q[mle_indices], y_3q[mle_indices], d=8)\n",
    "results_3q[0]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df698e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.70349999982864"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2q[0]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05e9551d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62491.66885833256"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_2q[0]*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39c01466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8925])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mle_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcc8d72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2q[mle_indices].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06d624c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_NN_fidelity_runtime_at_index(\n",
    "    X, y, test_index, lam=0.8, epochs=30, batch_size=64, validation_split=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Train on all samples except `test_index`, then evaluate inference & fidelity on that one sample.\n",
    "\n",
    "    Returns:\n",
    "      {\n",
    "        \"training_time_s\": float,\n",
    "        \"inference_time_single_s\": float,\n",
    "        \"fidelity_single\": float,\n",
    "        \"n_params_total\": int,\n",
    "        \"n_params_trainable\": int,\n",
    "        \"d\": int,\n",
    "        \"test_index_used\": int\n",
    "      }\n",
    "\n",
    "    Expects helper functions in scope:\n",
    "      - rho_to_alpha(rho)\n",
    "      - make_hybrid_loss(d, lam)\n",
    "      - alpha_to_rho_batch(d, A)\n",
    "      - fidelity(rho_pred, rho_gt)\n",
    "    \"\"\"\n",
    "    N = len(y)\n",
    "    if not (0 <= test_index < N):\n",
    "        raise ValueError(f\"test_index {test_index} is out of range [0, {N-1}].\")\n",
    "\n",
    "    d = y.shape[1]\n",
    "\n",
    "    mask = np.ones(N, dtype=bool)\n",
    "    mask[test_index] = False\n",
    "    X_train, y_train = X[mask], y[mask]\n",
    "    x_one = X[test_index:test_index+1]\n",
    "    y_one = y[test_index]\n",
    "\n",
    "    alphas_train = np.stack([rho_to_alpha(rho) for rho in y_train], axis=0)\n",
    "    N_alpha = alphas_train.shape[1]\n",
    "\n",
    "    # Model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(N_alpha),\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "        loss=make_hybrid_loss(d, lam=lam),\n",
    "    )\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    _ = model.fit(\n",
    "        X_train, alphas_train,\n",
    "        validation_split=validation_split,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True\n",
    "    )\n",
    "    training_time_s = time.perf_counter() - t0\n",
    "\n",
    "    _ = model.predict(x_one, verbose=0)\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    alpha_pred_one = model.predict(x_one, verbose=0)  # (1, N_alpha)\n",
    "    inference_time_single_s = time.perf_counter() - t1\n",
    "\n",
    "    rho_pred_one = alpha_to_rho_batch(d, alpha_pred_one)[0]\n",
    "    fidelity_single = float(fidelity(rho_pred_one, y_one))\n",
    "\n",
    "    n_params_total = int(model.count_params())\n",
    "    n_params_trainable = int(np.sum([np.prod(v.shape) for v in model.trainable_weights]))\n",
    "\n",
    "    return {\n",
    "        \"training_time_s\": training_time_s,\n",
    "        \"inference_time_single_s\": inference_time_single_s,\n",
    "        \"fidelity_single\": fidelity_single,\n",
    "        \"n_params_total\": n_params_total,\n",
    "        \"n_params_trainable\": n_params_trainable,\n",
    "        \"d\": int(d),\n",
    "        \"test_index_used\": int(test_index),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d2627b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.1134 - val_loss: 0.0106\n",
      "Epoch 2/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0169 - val_loss: 0.0084\n",
      "Epoch 3/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0139 - val_loss: 0.0080\n",
      "Epoch 4/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0127 - val_loss: 0.0073\n",
      "Epoch 5/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0119 - val_loss: 0.0069\n",
      "Epoch 6/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0113 - val_loss: 0.0067\n",
      "Epoch 7/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0110 - val_loss: 0.0064\n",
      "Epoch 8/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0109 - val_loss: 0.0062\n",
      "Epoch 9/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0107 - val_loss: 0.0067\n",
      "Epoch 10/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0105 - val_loss: 0.0062\n",
      "Epoch 11/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0103 - val_loss: 0.0062\n",
      "Epoch 12/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0103 - val_loss: 0.0059\n",
      "Epoch 13/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0101 - val_loss: 0.0062\n",
      "Epoch 14/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0101 - val_loss: 0.0060\n",
      "Epoch 15/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0100 - val_loss: 0.0064\n",
      "Epoch 16/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0099 - val_loss: 0.0060\n",
      "Epoch 17/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0099 - val_loss: 0.0062\n",
      "Epoch 18/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0098 - val_loss: 0.0060\n",
      "Epoch 19/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0098 - val_loss: 0.0059\n",
      "Epoch 20/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0059\n",
      "Epoch 21/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0060\n",
      "Epoch 22/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0097 - val_loss: 0.0057\n",
      "Epoch 23/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0096 - val_loss: 0.0060\n",
      "Epoch 24/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0058\n",
      "Epoch 25/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0058\n",
      "Epoch 26/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0059\n",
      "Epoch 27/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0059\n",
      "Epoch 28/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0060\n",
      "Epoch 29/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0095 - val_loss: 0.0060\n",
      "Epoch 30/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - loss: 0.0094 - val_loss: 0.0059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training_time_s': 61.21716291597113,\n",
       " 'inference_time_single_s': 0.023326750029809773,\n",
       " 'fidelity_single': 0.9871515984509145,\n",
       " 'n_params_total': 23968,\n",
       " 'n_params_trainable': 23640,\n",
       " 'd': 4,\n",
       " 'test_index_used': 8925}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_NN_fidelity_runtime_at_index(X_2q, y_2q, test_index = 8925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cfdeae22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - loss: 0.1094 - val_loss: 0.0096\n",
      "Epoch 2/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0112 - val_loss: 0.0065\n",
      "Epoch 3/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0085 - val_loss: 0.0059\n",
      "Epoch 4/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0075 - val_loss: 0.0056\n",
      "Epoch 5/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0069 - val_loss: 0.0055\n",
      "Epoch 6/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0066 - val_loss: 0.0053\n",
      "Epoch 7/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0062 - val_loss: 0.0051\n",
      "Epoch 8/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0058 - val_loss: 0.0049\n",
      "Epoch 9/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0055 - val_loss: 0.0048\n",
      "Epoch 10/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 11/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 12/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 13/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 14/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 15/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 16/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 17/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 18/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 19/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 20/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 21/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0048 - val_loss: 0.0046\n",
      "Epoch 22/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 23/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 24/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 25/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - loss: 0.0047 - val_loss: 0.0047\n",
      "Epoch 26/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 27/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 28/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 29/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 30/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0046 - val_loss: 0.0045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training_time_s': 277.1455034579849,\n",
       " 'inference_time_single_s': 0.024929082952439785,\n",
       " 'fidelity_single': 0.9834591053720451,\n",
       " 'n_params_total': 409504,\n",
       " 'n_params_trainable': 408048,\n",
       " 'd': 8,\n",
       " 'test_index_used': 8925}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_NN_fidelity_runtime_at_index(X_3q, y_3q, test_index = 8925)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d166963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
